{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63e155a",
   "metadata": {},
   "source": [
    "# Use a Logistic Regression to stack the 4 optimized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc539789",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import optuna\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6786c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/home/olli/Projects/Kaggle/ICR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d3059ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = 'train.csv'\n",
    "greek_csv = 'greeks.csv'\n",
    "test_csv = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a95147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = os.path.join(folder, train_csv)\n",
    "greek_csv_path = os.path.join(folder, greek_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ee660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_csv_path)\n",
    "df_g = pd.read_csv(greek_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffbff08",
   "metadata": {},
   "source": [
    "# Build a preprocessing pipeline (imputing, encoding, scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de0513e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['EJ']\n",
    "num_features = list(df.columns)\n",
    "for remove_value in ['Id', 'EJ', 'Class']:\n",
    "    num_features.remove(remove_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5deafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer_num = SimpleImputer(strategy='median')\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "968ba7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a standardscaler due to the outliers\n",
    "scaler_num = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d0b8535",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_cat = OneHotEncoder(sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df38f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('Num_Imputer', imputer_num),\n",
    "    ('Num_Scaler', scaler_num)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15ba0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('Cat_Imputer', imputer_cat),\n",
    "    ('Cat_Encoder', encoder_cat)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79780c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipe = ColumnTransformer([\n",
    "    ('Num_Pipe', num_pipeline, num_features),\n",
    "    ('Cat_Pipe', cat_pipeline, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54d3be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dfs for split\n",
    "X_DF = df.copy()\n",
    "X_DF['Alpha'] = df_g.Alpha\n",
    "X_DF['Gamma'] = df_g.Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b09f4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    nc = np.bincount(y_true)  # [num_class_0, num_class_1]\n",
    "    return log_loss(y_true, y_pred, sample_weight=1/nc[y_true], eps=1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef23e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd9352",
   "metadata": {},
   "source": [
    "# Define the optimized hyperparameters from the 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c96f25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {'n_estimators': 540,\n",
    "                 'max_depth': 3,\n",
    "                 'learning_rate': 0.037390264787993115,\n",
    "                 'gamma': 0.503401096426527,\n",
    "                 'min_child_weight': 6.0081952605965405,\n",
    "                 'max_delta_step': 5,\n",
    "                 'subsample': 0.9675791374036842,\n",
    "                 'colsample_bynode': 0.5385701026123967,\n",
    "                 'colsample_bytree': 0.548075108799538,\n",
    "                 'colsample_bylevel': 0.936147174988883,\n",
    "                 'reg_lambda': 0.020026716033135116,\n",
    "                 'scale_pos_weight': 6.362746795831511,\n",
    "                 'objective': 'binary:logistic'\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b61f699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {'C': 5,\n",
    "              'kernel': 'rbf',\n",
    "              'gamma': 0.017,\n",
    "              'coef0': -0.74,\n",
    "              'class_weight': {0: 1, 1: 7.6},\n",
    "              'probability': True\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "253f273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_tree_params = {'n_estimators': 85,\n",
    "                     'max_depth': 13,\n",
    "                     'min_samples_split': 0.03500662766420863,\n",
    "                     'min_samples_leaf': 0.002061652693906283,\n",
    "                     'min_weight_fraction_leaf': 0.00616078547424613,\n",
    "                     'max_features': 0.9999007675837779,\n",
    "                     'max_leaf_nodes': 31,\n",
    "                     'min_impurity_decrease': 0.0004818371664378797,\n",
    "                     'class_weight': {0: 1, 1: 4.9}\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "021d9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_params = {'n_estimators': 167,\n",
    "                        'criterion': 'entropy',\n",
    "                        'max_depth': 10,\n",
    "                        'min_samples_split': 0.05,\n",
    "                        'min_samples_leaf': 0.016,\n",
    "                        'min_weight_fraction_leaf': 0.0174,\n",
    "                        'max_features': 0.43112450096106114,\n",
    "                        'max_leaf_nodes': 28,\n",
    "                        'min_impurity_decrease': 0.00012,\n",
    "                        'bootstrap': False,\n",
    "                        'class_weight': {0: 1, 1: 9.747981332376273}\n",
    "                        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df5def",
   "metadata": {},
   "source": [
    "# Build a nested cross validation & save the pred / y for each inner fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58f1564b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d314dd9dc18455aa3205ed1a961cad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "seeds = list(range(0, 10))\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    \n",
    "    # cross-validation\n",
    "    cv_out = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    # use alpha+gamma to draw stratified samples\n",
    "    for train_index, valid_index in cv_out.split(X_DF, X_DF[['Alpha', 'Gamma']].copy()):\n",
    "\n",
    "        # take all columns for the features since the ColumnTransformer will only select the defined ones\n",
    "        X_train, y_train = X_DF.iloc[train_index], X_DF.loc[train_index, 'Class']\n",
    "        X_valid, y_valid = X_DF.iloc[valid_index], X_DF.loc[valid_index, 'Class']\n",
    "        \n",
    "        # now train 20 models (4 models * 5 folds) and use the trained ones on the outer hold-out fold\n",
    "        xgb = xgboost.XGBClassifier(**xgb_params)\n",
    "        svm = SVC(**svm_params)\n",
    "        extra_tree = ExtraTreesClassifier(**extra_tree_params)\n",
    "        random_forest = RandomForestClassifier(**random_forest_params)\n",
    "        \n",
    "        # use defined pipeline\n",
    "        X_train = preprocess_pipe.fit_transform(X_train)\n",
    "        X_valid = preprocess_pipe.transform(X_valid)  # no fit\n",
    "\n",
    "        y_train = np.array(y_train)\n",
    "        y_valid = np.array(y_valid)        \n",
    "        \n",
    "        xgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], \\\n",
    "                early_stopping_rounds=196, verbose=0)\n",
    "        svm.fit(X_train, y_train)\n",
    "        extra_tree.fit(X_train, y_train)\n",
    "        random_forest.fit(X_train, y_train)\n",
    "        \n",
    "        models = [xgb, svm, extra_tree, random_forest]\n",
    "        \n",
    "        # append each predictions from the 4 models, then average the probabilities\n",
    "        predictions = []\n",
    "        \n",
    "        for model in models:\n",
    "            pred = model.predict_proba(X_valid)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        #average the predictions\n",
    "        predictions = np.array(predictions).mean(axis=0)\n",
    "        \n",
    "        # calculate score for this validation fold from the averaged predictions\n",
    "        score = balanced_log_loss(y_valid, predictions)\n",
    "        scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c544897b",
   "metadata": {},
   "source": [
    "#### 10 seeds * 5-fold-cv should make 50 scores again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "188e2a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e13e1a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 0.3085278689596406\n"
     ]
    }
   ],
   "source": [
    "print(f'Final Score: {np.array(scores).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163c9155",
   "metadata": {},
   "source": [
    "#### Final Score did not get better then with the xgb / random forest alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5882acca",
   "metadata": {},
   "source": [
    "### Since the SVM had by far the worst score try an ensemble without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baf865dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c69535e39f0423b8fe0c9caf08195a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_2 = []\n",
    "\n",
    "seeds = list(range(0, 10))\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    \n",
    "    # cross-validation\n",
    "    cv_out = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    # use alpha+gamma to draw stratified samples\n",
    "    for train_index, valid_index in cv_out.split(X_DF, X_DF[['Alpha', 'Gamma']].copy()):\n",
    "\n",
    "        # take all columns for the features since the ColumnTransformer will only select the defined ones\n",
    "        X_train, y_train = X_DF.iloc[train_index], X_DF.loc[train_index, 'Class']\n",
    "        X_valid, y_valid = X_DF.iloc[valid_index], X_DF.loc[valid_index, 'Class']\n",
    "        \n",
    "        # now train 20 models (4 models * 5 folds) and use the trained ones on the outer hold-out fold\n",
    "        xgb = xgboost.XGBClassifier(**xgb_params)\n",
    "        extra_tree = ExtraTreesClassifier(**extra_tree_params)\n",
    "        random_forest = RandomForestClassifier(**random_forest_params)\n",
    "        \n",
    "        # use defined pipeline\n",
    "        X_train = preprocess_pipe.fit_transform(X_train)\n",
    "        X_valid = preprocess_pipe.transform(X_valid)  # no fit\n",
    "\n",
    "        y_train = np.array(y_train)\n",
    "        y_valid = np.array(y_valid)        \n",
    "        \n",
    "        xgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], \\\n",
    "                early_stopping_rounds=196, verbose=0)\n",
    "        extra_tree.fit(X_train, y_train)\n",
    "        random_forest.fit(X_train, y_train)\n",
    "        \n",
    "        models = [xgb, extra_tree, random_forest]\n",
    "        \n",
    "        # append each predictions from the 4 models, then average the probabilities\n",
    "        predictions = []\n",
    "        \n",
    "        for model in models:\n",
    "            pred = model.predict_proba(X_valid)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        #average the predictions\n",
    "        predictions = np.array(predictions).mean(axis=0)\n",
    "        \n",
    "        # calculate score for this validation fold from the averaged predictions\n",
    "        score = balanced_log_loss(y_valid, predictions)\n",
    "        scores_2.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7da0d81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score without SVM: 0.29423840919388783\n"
     ]
    }
   ],
   "source": [
    "print(f'Final Score without SVM: {np.array(scores_2).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7732c7",
   "metadata": {},
   "source": [
    "### Only the XGB and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73ed0a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d832882720741469d89027d28850cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_3 = []\n",
    "\n",
    "seeds = list(range(0, 10))\n",
    "\n",
    "for seed in tqdm(seeds):\n",
    "    \n",
    "    # cross-validation\n",
    "    cv_out = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    # use alpha+gamma to draw stratified samples\n",
    "    for train_index, valid_index in cv_out.split(X_DF, X_DF[['Alpha', 'Gamma']].copy()):\n",
    "\n",
    "        # take all columns for the features since the ColumnTransformer will only select the defined ones\n",
    "        X_train, y_train = X_DF.iloc[train_index], X_DF.loc[train_index, 'Class']\n",
    "        X_valid, y_valid = X_DF.iloc[valid_index], X_DF.loc[valid_index, 'Class']\n",
    "        \n",
    "        # now train 20 models (4 models * 5 folds) and use the trained ones on the outer hold-out fold\n",
    "        xgb = xgboost.XGBClassifier(**xgb_params)\n",
    "        random_forest = RandomForestClassifier(**random_forest_params)\n",
    "        \n",
    "        # use defined pipeline\n",
    "        X_train = preprocess_pipe.fit_transform(X_train)\n",
    "        X_valid = preprocess_pipe.transform(X_valid)  # no fit\n",
    "\n",
    "        y_train = np.array(y_train)\n",
    "        y_valid = np.array(y_valid)        \n",
    "        \n",
    "        xgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], \\\n",
    "                early_stopping_rounds=196, verbose=0)\n",
    "        random_forest.fit(X_train, y_train)\n",
    "        \n",
    "        models = [xgb, random_forest]\n",
    "        \n",
    "        # append each predictions from the 4 models, then average the probabilities\n",
    "        predictions = []\n",
    "        \n",
    "        for model in models:\n",
    "            pred = model.predict_proba(X_valid)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        #average the predictions\n",
    "        predictions = np.array(predictions).mean(axis=0)\n",
    "        \n",
    "        # calculate score for this validation fold from the averaged predictions\n",
    "        score = balanced_log_loss(y_valid, predictions)\n",
    "        scores_3.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9232aa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score (XGB & Random Forest): 0.2720216365485635\n"
     ]
    }
   ],
   "source": [
    "print(f'Final Score (XGB & Random Forest): {np.array(scores_3).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e7286f",
   "metadata": {},
   "source": [
    "# Next: Use a Logistic Regression on these individual models (stacking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabular",
   "language": "python",
   "name": "tabular"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
